)
# find the percentage the member agreed with a vote
votes_info <- fromJSON(votes_info_body)$results
expanded_votes_info <- votes_info$votes %>%
as.data.frame() %>%
flatten() %>%
select(position, result)
percentage_agreed <- round(nrow(filter(
expanded_votes_info, position == "Yes" & result == "Passed" |
position == "No" & result == "Failed"
)) / nrow(expanded_votes_info) * 100)
lintr:::addin_lint()
#Source in relevant scripts
source("civic-info.R")
source("propublica.R")
# Load the knitr library
library(knitr)
#Display simple dataframe from civic api info
kable(full_table)
#Create simple bar plots from sourced information
#barplot(main = "Representatives by Gender", xlab = "# of Representatives",
#horiz = TRUE, c(nrow(pro_results) - females, females),
#names.arg = c("Males", "Females"))
#barplot(main = "Representatives by Party", xlab = "# of Representatives",
#horiz = TRUE, c(nrow(pro_results) - democrats, democrats),
#names.arg = c("Republicans", "Democrats"))
reps_by_gen
reps_by_party
# set-up
library("httr")
library(jsonlite)
source("api-keys.R")
source("civic-info.R")
# set up for query full endpoint
state <- civic_parsed_data$normalizedInput$state
chamber <- "house"
# create endpoint
pro_resource <- "/members"
pro_base_url <- "https://api.propublica.org/congress/v1"
pro_endpoint <- paste0(
pro_base_url, pro_resource, "/",
chamber, "/", state, "/current.json"
)
# MakeGET request
pro_response <- GET(pro_endpoint, add_headers("X-API-Key" = pro_apikey))
# Extraxt content from response
pro_body <- content(pro_response, "text")
pro_parsed_data <- fromJSON(pro_body)
# set up barplots to show in index
pro_results <- pro_parsed_data$results
females <- filter(pro_results, gender == "F") %>% nrow()
democrats <- filter(pro_results, party == "D") %>% nrow()
# setup for specified member endpoint and votes endpoint
library(eeptools)
member <- pro_results[1, 1]
# create endpoints for member and member votes information
mem_info_endpoint <- paste0(pro_base_url, pro_resource, "/", member, ".json")
votes_info_endpoint <- paste0(
pro_base_url, pro_resource, "/", member,
"/votes.json"
)
# MakeGET requests
mem_info_response <- GET(
mem_info_endpoint,
add_headers("X-API-Key" = pro_apikey)
)
votes_info_response <- GET(
votes_info_endpoint,
add_headers("X-API-Key" = pro_apikey)
)
# Extraxt contents from responses
mem_info_body <- content(mem_info_response, "text")
votes_info_body <- content(votes_info_response, "text")
mem_info <- fromJSON(mem_info_body)$results
# specify member age and twitter account for use in index
mem_age <- floor(age_calc(as.Date(mem_info$date_of_birth),
Sys.Date(),
units = "years"
))
mem_info$twitter_account <- paste0(
"[", mem_info$twitter_account, "]
(https://www.twitter.com/",
mem_info$twitter_account, ")"
)
# find the percentage the member agreed with a vote
votes_info <- fromJSON(votes_info_body)$results
expanded_votes_info <- votes_info$votes %>%
as.data.frame() %>%
flatten() %>%
select(position, result)
percentage_agreed <- round(nrow(filter(
expanded_votes_info, position == "Yes" & result == "Passed" |
position == "No" & result == "Failed"
)) / nrow(expanded_votes_info) * 100)
barplot(main = "Representatives by Gender", xlab = "# of Representatives",
horiz = TRUE, c(nrow(pro_results) - females, females),
names.arg = c("Males", "Females"))
barplot(main = "Representatives by Party", xlab = "# of Representatives",
horiz = TRUE, c(nrow(pro_results) - democrats, democrats),
names.arg = c("Republicans", "Democrats"))
# set-up
library("httr")
library(jsonlite)
source("api-keys.R")
source("civic-info.R")
# set up for query full endpoint
state <- civic_parsed_data$normalizedInput$state
chamber <- "house"
# create endpoint
pro_resource <- "/members"
pro_base_url <- "https://api.propublica.org/congress/v1"
pro_endpoint <- paste0(
pro_base_url, pro_resource, "/",
chamber, "/", state, "/current.json"
)
# MakeGET request
pro_response <- GET(pro_endpoint, add_headers("X-API-Key" = pro_apikey))
# Extraxt content from response
pro_body <- content(pro_response, "text")
pro_parsed_data <- fromJSON(pro_body)
# set up barplots to show in index
pro_results <- pro_parsed_data$results
females <- filter(pro_results, gender == "F") %>% nrow()
democrats <- filter(pro_results, party == "D") %>% nrow()
# setup for specified member endpoint and votes endpoint
library(eeptools)
member <- pro_results[1, 1]
# create endpoints for member and member votes information
mem_info_endpoint <- paste0(pro_base_url, pro_resource, "/", member, ".json")
votes_info_endpoint <- paste0(
pro_base_url, pro_resource, "/", member,
"/votes.json"
)
# MakeGET requests
mem_info_response <- GET(
mem_info_endpoint,
add_headers("X-API-Key" = pro_apikey)
)
votes_info_response <- GET(
votes_info_endpoint,
add_headers("X-API-Key" = pro_apikey)
)
# Extraxt contents from responses
mem_info_body <- content(mem_info_response, "text")
votes_info_body <- content(votes_info_response, "text")
mem_info <- fromJSON(mem_info_body)$results
# specify member age and twitter account for use in index
mem_age <- floor(age_calc(as.Date(mem_info$date_of_birth),
Sys.Date(),
units = "years"
))
mem_info$twitter_account <- paste0(
"[", mem_info$twitter_account, "](https://www.twitter.com/",
mem_info$twitter_account, ")"
)
# find the percentage the member agreed with a vote
votes_info <- fromJSON(votes_info_body)$results
expanded_votes_info <- votes_info$votes %>%
as.data.frame() %>%
flatten() %>%
select(position, result)
percentage_agreed <- round(nrow(filter(
expanded_votes_info, position == "Yes" & result == "Passed" |
position == "No" & result == "Failed"
)) / nrow(expanded_votes_info) * 100)
lintr:::addin_lint()
lintr:::addin_lint()
lintr:::addin_lint()
lintr:::addin_lint()
lintr:::addin_lint()
lintr:::addin_lint()
lintr:::addin_lint()
# Exercise 1: Building a user interface
# Load the `shiny` package (install it in the R terminal if you haven't already)
library(shiny)
# Define a new `ui` variable. This variable should be assigned a `fluidPage()`
# layout, which should be passed the following:
ui <- fluidPage(
# A top level header: "First Shiny Website"
h1("First Shiny Website"),
# A Paragraph, describing your excitement: include some `strong` text
p(
"This is my", strong("first"), "shiny project, and I feel..."
),
# An image with this `src`
# https://media2.giphy.com/media/l3q2Ip7FrmPE33EiI/giphy.gif
img(
"",
src = "https://media2.giphy.com/media/l3q2Ip7FrmPE33EiI/giphy.gif"
),
# Another paragraph about a slider: include some `em` text
p("I can't wait to get", em("sliders"), "like these to work."),
# A slider with an appropriate label, min of 10, max of 100, value of 50
sliderInput(
"slider",
"My First Slider", min = 0, max = 100, value = 50, step = 10
)
)
# Define a `server` function that accepts an input and an output
# At this point, don't do anything in the function
# This function should perform the following:
server <- function(input, output) {
}
# Create a new `shinyApp()` using the above ui and server
shinyApp(ui = ui, server = server)
library(httr)
library(jsonlite)
call <- paste0("https://traffic.cit.api.here.com/traffic/6.2/incidents.json?app_id=3XM2tXG9AcbdYGZSYQrN&app_code=HRqp1gOmX1YOMRjUs4_o9Q&bbox=52.516,13.355;52.511,13.400")
get <- GET(call)
class(get_tweets)
class(get)
get_info <- content(get, "text")  # Convert to "character"
traffic_data <- fromJSON(get_info, flatten = TRUE) # Flatten into list
View(traffic_data)
class(traffic_data)
traffic_data <- fromJSON(get_info, flatten = TRUE) # Flatten into list
class(traffic_data)
## Process into a data table
get_info <- content(get, "text")  # Convert to "character"
class(get_info)
print(get_info)
names(traffic_data)
View(traffic_data$TIMESTAMP)
View(traffic_data$VERSION)
View(traffic_data$TRAFFIC_ITEMS)
View(traffic_data$TRAFFIC_ITEMS$TRAFFIC_ITEM)
View(traffic_data$diagnostic)
call <- paste0("https://traffic.cit.api.here.com/traffic/6.2/flow.json?app_id=3XM2tXG9AcbdYGZSYQrN&app_code=HRqp1gOmX1YOMRjUs4_o9Q&bbox=52.516,13.355;52.511,13.400")
get <- GET(call)
class(get)
## Process into a data table
get_info <- content(get, "text")  # Convert to "character"
traffic_data <- fromJSON(get_info, flatten = TRUE) # Flatten into list
names(traffic_data)
View(traffic_data$RWS)
View(traffic_data$MAP_VERSION)
View(traffic_data$VERSION)
View(traffic_data$UNITS)
View(traffic_data)
library(httr)
library(jsonlite)
call <- paste0("https://traffic.cit.api.here.com/traffic/6.2/incident.json?app_id=3XM2tXG9AcbdYGZSYQrN&app_code=HRqp1gOmX1YOMRjUs4_o9Q&bbox=52.516,13.355;52.511,13.400")
get <- GET(call)
class(get)
## Process into a data table
get_info <- content(get, "text")  # Convert to "character"
traffic_data <- fromJSON(get_info, flatten = TRUE) # Flatten into list
View(traffic_data$TRAFFIC_ITEMS$TRAFFIC_ITEM)
# Setup
library(httr)
library(jsonlite)
library(shiny)
library(shinythemes)
library(dplyr)
library(ggplot2)
library(wordcloud2)
library("tm")
library("SnowballC")
library("RColorBrewer")
source("script/api-key.R")
# Define the server
base     <- "https://rest.coinapi.io/"
endpoint <- "/v1/twitter/latest?"
api_key  <- paste0("apikey=", api)
## Create
call <- paste0(base, endpoint, api_key)
# call
## Retrieve data via GET call
get_tweets <- GET(call)
## Returns class "response"
class(get_tweets)
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
twitter_data <- fromJSON(get_tweets_text, flatten = TRUE) # Flatten into list
tweets <- twitter_data %>%
select(text)
text <- ""
for (row in 1:100) {
text <- paste(text, tweets[row,])
}
data <- Corpus(VectorSource(text))
#inspect(data)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
data <- tm_map(data, toSpace, " ' ")
# Convert the text to lower case
data <- tm_map(data, content_transformer(tolower))
# Remove numbers
data <- tm_map(data, removeNumbers)
# Remove english common stopwords
data <- tm_map(data, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
data <- tm_map(data, removeWords, c("RT", "@"))
# Remove punctuations
data <- tm_map(data, removePunctuation)
# Eliminate extra white spaces
data <- tm_map(data, stripWhitespace)
dtm <- TermDocumentMatrix(data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
for (row in 1:nrow(d)) {
if(grepl("???", d[row,1]) || grepl("â", d[row,1]) || grepl("S", d[row,1]) || grepl("ç", d[row,1]) || grepl("Y", d[row,1]) || grepl("æ", d[row,1]) || grepl("https", d[row,1])) {
d <- d[-row,]
}
}
#output$max_freq <- reactive({
# max(d$freq)
#})
server <- function(input, output) {
output$plot <- renderPlot({
wordcloud(words = d$word, freq = d$freq, min.freq = 3,
max.words=350, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
})
}
# Call the server
shinyServer(server)
setwd("C:/Users/Dayoung Cheong/Desktop/INFO-201-Final-Project-AD5")
# Setup
library(httr)
library(jsonlite)
library(shiny)
library(shinythemes)
library(dplyr)
library(ggplot2)
library(wordcloud2)
library("tm")
library("SnowballC")
library("RColorBrewer")
source("script/api-key.R")
# Define the server
base     <- "https://rest.coinapi.io/"
endpoint <- "/v1/twitter/latest?"
api_key  <- paste0("apikey=", api)
## Create
call <- paste0(base, endpoint, api_key)
# call
## Retrieve data via GET call
get_tweets <- GET(call)
## Returns class "response"
class(get_tweets)
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
twitter_data <- fromJSON(get_tweets_text, flatten = TRUE) # Flatten into list
tweets <- twitter_data %>%
select(text)
text <- ""
for (row in 1:100) {
text <- paste(text, tweets[row,])
}
data <- Corpus(VectorSource(text))
#inspect(data)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
data <- tm_map(data, toSpace, " ' ")
# Convert the text to lower case
data <- tm_map(data, content_transformer(tolower))
# Remove numbers
data <- tm_map(data, removeNumbers)
# Remove english common stopwords
data <- tm_map(data, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
data <- tm_map(data, removeWords, c("RT", "@"))
# Remove punctuations
data <- tm_map(data, removePunctuation)
# Eliminate extra white spaces
data <- tm_map(data, stripWhitespace)
dtm <- TermDocumentMatrix(data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
for (row in 1:nrow(d)) {
if(grepl("???", d[row,1]) || grepl("â", d[row,1]) || grepl("S", d[row,1]) || grepl("ç", d[row,1]) || grepl("Y", d[row,1]) || grepl("æ", d[row,1]) || grepl("https", d[row,1])) {
d <- d[-row,]
}
}
#output$max_freq <- reactive({
# max(d$freq)
#})
server <- function(input, output) {
output$plot <- renderPlot({
wordcloud(words = d$word, freq = d$freq, min.freq = 3,
max.words=350, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
})
}
# Call the server
shinyServer(server)
wordcloud(words = d$word, freq = d$freq, min.freq = 3,
max.words=350, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(wordcloud)
wordcloud(words = d$word, freq = d$freq, min.freq = 3,
max.words=350, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Setup
library(httr)
library(jsonlite)
library(shiny)
library(shinythemes)
library(dplyr)
library(ggplot2)
library(wordcloud)
library("tm")
library("SnowballC")
library("RColorBrewer")
source("script/api-key.R")
# Define the server
base     <- "https://rest.coinapi.io/"
endpoint <- "/v1/twitter/latest?"
api_key  <- paste0("apikey=", api)
## Create
call <- paste0(base, endpoint, api_key)
# call
## Retrieve data via GET call
get_tweets <- GET(call)
## Returns class "response"
class(get_tweets)
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
twitter_data <- fromJSON(get_tweets_text, flatten = TRUE) # Flatten into list
tweets <- twitter_data %>%
select(text)
text <- ""
for (row in 1:100) {
text <- paste(text, tweets[row,])
}
data <- Corpus(VectorSource(text))
#inspect(data)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
data <- tm_map(data, toSpace, " ' ")
# Convert the text to lower case
data <- tm_map(data, content_transformer(tolower))
# Remove numbers
data <- tm_map(data, removeNumbers)
# Remove english common stopwords
data <- tm_map(data, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
data <- tm_map(data, removeWords, c("RT", "@"))
# Remove punctuations
data <- tm_map(data, removePunctuation)
# Eliminate extra white spaces
data <- tm_map(data, stripWhitespace)
dtm <- TermDocumentMatrix(data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
for (row in 1:nrow(d)) {
if(grepl("???", d[row,1]) || grepl("â", d[row,1]) || grepl("S", d[row,1]) || grepl("ç", d[row,1]) || grepl("Y", d[row,1]) || grepl("æ", d[row,1]) || grepl("https", d[row,1])) {
d <- d[-row,]
}
}
#output$max_freq <- reactive({
# max(d$freq)
#})
server <- function(input, output) {
output$plot <- renderPlot({
wordcloud(words = d$word, freq = d$freq, min.freq = 3,
max.words=350, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
})
}
# Call the server
#shinyServer(server)
d
base     <- "https://rest.coinapi.io/"
endpoint <- "/v1/twitter/latest?"
api_key  <- paste0("apikey=", api)
source("script/api-key.R")
api_key  <- paste0("apikey=", api)
## Create
call <- paste0(base, endpoint, api_key)
# call
## Retrieve data via GET call
get_tweets <- GET(call)
## Returns class "response"
class(get_tweets)
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
library(httr)
library(jsonlite)
library(shiny)
library(shinythemes)
library(dplyr)
library(ggplot2)
library(wordcloud)
library("tm")
library("SnowballC")
library("RColorBrewer")
source("script/api-key.R")
base     <- "https://rest.coinapi.io/"
endpoint <- "/v1/twitter/latest?"
api_key  <- paste0("apikey=", api)
## Create
call <- paste0(base, endpoint, api_key)
# call
## Retrieve data via GET call
get_tweets <- GET(call)
## Returns class "response"
class(get_tweets)
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
?content
## Process into a data table
get_tweets_text <- content(get_tweets, type = "text")  # Convert to "character"
## Process into a data table
get_tweets_text <- content(get_tweets, "text")  # Convert to "character"
twitter_data <- fromJSON(get_tweets_text, flatten = TRUE) # Flatten into list
